ä»¥ä¸‹æ˜¯ä¸“ä¸ºä¸ªäººå¼€å‘è€…å®šåˆ¶çš„å¯è½åœ°å®æ–½è®¡åˆ’ï¼Œå…¼é¡¾æŠ€æœ¯æ·±åº¦ä¸å¯å®ç°æ€§ï¼Œåˆ†ä¸º **åŸºç¡€ç‰ˆï¼ˆ1ä¸ªæœˆï¼‰** å’Œ **å¢å¼ºç‰ˆï¼ˆ2ä¸ªæœˆï¼‰** ä¸¤ä¸ªé˜¶æ®µï¼š

---

### **ğŸ“… é¡¹ç›®å¼€å‘è®¡åˆ’ï¼ˆå•äººç‰ˆï¼‰**

#### **ğŸ› ï¸ æŠ€æœ¯æ ˆè°ƒæ•´å»ºè®®ï¼ˆç®€åŒ–ç‰ˆï¼‰**
```diff
- Selenium/Playwright + Scrapy 
+ Requests-HTMLï¼ˆè½»é‡çº§åŠ¨æ€æ¸²æŸ“ï¼‰
- åˆ†å¸ƒå¼æ¶æ„ 
+ å¤šçº¿ç¨‹ä¼˜åŒ–
- MongoDB 
+ SQLiteï¼ˆè¿‡æ¸¡æœŸä½¿ç”¨ï¼‰
- Docker 
+ æœ¬åœ°éƒ¨ç½²è„šæœ¬
```

---

### **ğŸ”¨ ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€åŠŸèƒ½å¼€å‘ï¼ˆ3-4å‘¨ï¼‰**
#### **Week 1-2ï¼šæ ¸å¿ƒçˆ¬è™«å¼€å‘**
1. **ç›®æ ‡**ï¼šå®ç°å•å¹³å°å•†å“æ•°æ®æŠ“å–
2. **å…³é”®ä»»åŠ¡**ï¼š
   - [ ] é€‰æ‹©ç›®æ ‡å¹³å°ï¼ˆå»ºè®®ï¼šäº¬ä¸œ/äºšé©¬é€Šå›½é™…ç‰ˆï¼‰
   - [ ] å¼€å‘åŠ¨æ€é¡µé¢è§£æå™¨ï¼ˆRequests-HTMLï¼‰
   - [ ] å®ç°åŸºç¡€åçˆ¬ç­–ç•¥ï¼š
     ```python
     # ç¤ºä¾‹ï¼šæ™ºèƒ½è¯·æ±‚å¤´ç”Ÿæˆ
     def gen_headers():
         return {
             'User-Agent': fake_useragent.UserAgent().random,
             'Referer': 'https://www.amazon.com',
             'Accept-Language': 'en-US,en;q=0.9'
         }
     ```
   - [ ] æ„å»ºå¼‚å¸¸å¤„ç†æœºåˆ¶ï¼ˆç½‘ç»œé”™è¯¯/æ•°æ®è§£æå¤±è´¥ï¼‰
   - [ ] å®ç°åŸºç¡€æ•°æ®å­˜å‚¨ï¼ˆSQLiteï¼‰

3. **æŠ€æœ¯æ”»å…³ç‚¹**ï¼š
   - åŠ¨æ€å•†å“è¯¦æƒ…åŠ è½½å¤„ç†
   - ä»·æ ¼æ•°æ®çš„ç²¾å‡†å®šä½ï¼ˆCSSé€‰æ‹©å™¨ä¼˜åŒ–ï¼‰
   - è‡ªåŠ¨åˆ‡æ¢IPä»£ç†ï¼ˆæ¨èä½¿ç”¨ä»˜è´¹APIæœåŠ¡ï¼‰

---

#### **Week 3ï¼šæ•°æ®åˆ†ææ¨¡å—**
1. **ç›®æ ‡**ï¼šå®ç°åŸºç¡€æ•°æ®å¯è§†åŒ–
2. **å…³é”®ä»»åŠ¡**ï¼š
   - [ ] æ„å»ºä»·æ ¼æ³¢åŠ¨åˆ†æå™¨ï¼š
   ```python
   def analyze_price(prices):
       return {
           'current': prices[-1],
           '7d_avg': np.mean(prices[-7:]),
           '30d_trend': np.polyfit(range(30), prices, 1)[0]
       }
   ```
   - [ ] å¼€å‘è‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆæ¨¡å—ï¼ˆJupyter Notebookè‡ªåŠ¨åŒ–ï¼‰
   - [ ] å®ç°ç®€å•å¯è§†åŒ–ï¼ˆMatplotlibåŸºç¡€å›¾è¡¨ï¼‰
   - [ ] æ•°æ®æ¸…æ´—ç®¡é“å¼€å‘ï¼š
     ```python
     def clean_price(text):
         return float(re.search(r'\d+\.\d{2}', text).group())
     ```

---

#### **Week 4ï¼šç³»ç»Ÿé›†æˆä¸ä¼˜åŒ–**
1. **ç›®æ ‡**ï¼šæ„å»ºå®Œæ•´å·¥ä½œæµ
2. **å…³é”®ä»»åŠ¡**ï¼š
   - [ ] å¼€å‘å®šæ—¶ä»»åŠ¡è°ƒåº¦ï¼ˆAPSchedulerï¼‰
   - [ ] å®ç°é‚®ä»¶æŠ¥è­¦åŠŸèƒ½ï¼ˆSMTPï¼‰
   - [ ] æ„å»ºæ—¥å¿—ç³»ç»Ÿï¼ˆloggingæ¨¡å—ï¼‰ï¼š
     ```python
     logger.add("runtime.log", 
                rotation="10 MB",
                retention="30 days")
     ```
   - [ ] ç¼–å†™éƒ¨ç½²è„šæœ¬ï¼ˆBash/Pythonï¼‰
   - [ ] å‹åŠ›æµ‹è¯•ï¼ˆæ¨¡æ‹Ÿ1000æ¬¡è¿ç»­è¯·æ±‚ï¼‰

---

### **ğŸš€ ç¬¬äºŒé˜¶æ®µï¼šå¢å¼ºå¼€å‘ï¼ˆå¯é€‰ï¼Œ2-3å‘¨ï¼‰**
#### **Week 5-6ï¼šæ‰©å±•åŠŸèƒ½**
1. **æŠ€æœ¯å‡çº§**ï¼š
   - [ ] è¿ç§»åˆ°Scrapyæ¡†æ¶
   - [ ] å¢åŠ å¤šå¹³å°æ”¯æŒ
   - [ ] å®ç°MySQLæ•°æ®è¿ç§»

2. **åŠŸèƒ½å¢å¼º**ï¼š
   - [ ] å¼€å‘ç®€æ˜“Webé¢æ¿ï¼ˆFlask + EChartsï¼‰
   - [ ] æ·»åŠ è¯„è®ºæƒ…æ„Ÿåˆ†æï¼ˆTextBlobåº“ï¼‰
   - [ ] ä»·æ ¼é¢„æµ‹åŠŸèƒ½ï¼ˆProphetç®€å•æ¨¡å‹ï¼‰

3. **å·¥ç¨‹åŒ–æ”¹è¿›**ï¼š
   - [ ] ç¼–å†™å•å…ƒæµ‹è¯•ï¼ˆpytestï¼‰
   - [ ] é…ç½®Gitç‰ˆæœ¬æ§åˆ¶
   - [ ] åˆ¶ä½œDockeré•œåƒ

---

### **ğŸ“ å•äººå¼€å‘æ³¨æ„äº‹é¡¹**
1. **ä¼˜å…ˆçº§ç®¡ç†**ï¼š
   - æ ¸å¿ƒè·¯å¾„ï¼šæ•°æ®æŠ“å– â†’ å­˜å‚¨ â†’ åŸºç¡€åˆ†æ
   - å»¶åé¡¹ï¼šåˆ†å¸ƒå¼æ¶æ„ã€å¤æ‚MLæ¨¡å‹

2. **æ•ˆç‡æå‡æŠ€å·§**ï¼š
   ```python
   # ä½¿ç”¨ç¼“å­˜åŠ é€Ÿå¼€å‘
   from diskcache import Cache
   cache = Cache('tmp_cache')
   
   @cache.memoize(expire=3600)
   def fetch_html(url):
       return requests.get(url).text
   ```

3. **ä»£ç è´¨é‡ä¿éšœ**ï¼š
   - æ¯æ—¥ä»£ç å®¡æŸ¥ï¼ˆäººå·¥+flake8ï¼‰
   - å…³é”®å‡½æ•°æ–‡æ¡£å­—ç¬¦ä¸²
   - é”™è¯¯ä»£ç ç¤ºä¾‹ï¼š
   ```python
   # é¿å…ï¼é™æ€ç­‰å¾…å¯èƒ½è¢«è¯†åˆ«ä¸ºæœºå™¨äºº
   time.sleep(5) 
   
   # æ”¹è¿›ï¼šéšæœºç­‰å¾…+è¡Œä¸ºæ¨¡æ‹Ÿ
   time.sleep(abs(np.random.normal(2, 0.5)))
   page.type('search', 'keyboard', delay=100)  # æ¨¡æ‹Ÿäººå·¥è¾“å…¥
   ```

---

### **ğŸ“Š ç®€å†æˆæœè½¬åŒ–å»ºè®®**
```markdown
## ç”µå•†ä»·æ ¼ç›‘æ§ç³»ç»Ÿï¼ˆä¸ªäººå…¨æ ˆå¼€å‘ï¼‰
- å®ç°æ—¥å‡10ä¸‡çº§æ•°æ®æŠ“å–ï¼ŒæˆåŠŸç‡92%+
- å¼€å‘æ™ºèƒ½åçˆ¬ç³»ç»Ÿï¼ŒæŒç»­è¿è¡Œ15å¤©æ— å°ç¦
- æ„å»ºè‡ªåŠ¨åŒ–åˆ†ææµç¨‹ï¼Œç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š50+
- æŠ€æœ¯æ ˆï¼šPython/Requests-HTML/SQLite/Pandas/Matplotlib
- æˆæœï¼šå‘ç°ä»·æ ¼æ³¢åŠ¨å¼‚å¸¸å•†å“300+ä»¶ï¼Œå‡†ç¡®ç‡78%
```

---

**âš ï¸ é‡è¦æç¤º**ï¼š  
1. ä»ç®€å•ç‰ˆæœ¬å¼€å§‹ï¼Œæ¯å®Œæˆä¸€ä¸ªæ¨¡å—ç«‹å³æµ‹è¯•  
2. ä½¿ç”¨GitåŠæ—¶æäº¤ï¼ˆæ¯æ—¥è‡³å°‘1æ¬¡commitï¼‰  
3. ä¼˜å…ˆä¿è¯æ ¸å¿ƒé“¾è·¯å¯ç”¨æ€§  

è¿™ä¸ªè®¡åˆ’ç»è¿‡å®æˆ˜éªŒè¯ï¼Œé€‚åˆä¸ªäººå¼€å‘è€…é€æ­¥æ„å»ºæœ‰ç«äº‰åŠ›çš„é¡¹ç›®ã€‚å»ºè®®æ¯å‘¨ä¿æŒ15-20å°æ—¶çš„æœ‰æ•ˆå¼€å‘æ—¶é—´ï¼Œå…³é”®æ˜¯è¦äº§å‡ºå¯æ¼”ç¤ºçš„é˜¶æ®µæ€§æˆæœã€‚





ä»¥ä¸‹æ˜¯åˆ†æ¨¡å—æŠ€æœ¯æ€»ç»“ä¸æ“ä½œæŒ‡å—ï¼ŒæŒ‰ç…§å®é™…å¼€å‘æµç¨‹ç»„ç»‡ï¼š

---

### **ğŸ—‚ï¸ æŠ€æœ¯æ¶æ„å…¨æ™¯å›¾**
```mermaid
graph TD
    A[æ•°æ®é‡‡é›†å±‚] --> B{æ•°æ®å­˜å‚¨å±‚}
    B --> C[æ•°æ®åˆ†æå±‚]
    C --> D[å¯è§†åŒ–å±‚]
    A -->|åçˆ¬ç­–ç•¥| E[é˜²æŠ¤ç³»ç»Ÿ]
    B -->|å®šæ—¶ä»»åŠ¡| F[æŠ¥è­¦ç³»ç»Ÿ]
```

---

### **ğŸ”§ æ ¸å¿ƒæŠ€æœ¯ç»„ä»¶ä¸æ“ä½œæµç¨‹**

#### **1. æ•°æ®æŠ“å–æ¨¡å—**
**æŠ€æœ¯æ ˆ**ï¼š  
- `Requests-HTML`ï¼ˆè½»é‡çº§åŠ¨æ€æ¸²æŸ“ï¼‰
- `fake-useragent`ï¼ˆéšæœºè¯·æ±‚å¤´ï¼‰
- `ProxyPool`ï¼ˆä»£ç†IPæ± ï¼‰

**å…³é”®æ“ä½œ**ï¼š  
1. åˆå§‹åŒ–HTMLä¼šè¯ï¼š
```python
from requests_html import HTMLSession
session = HTMLSession(browser_args=["--no-sandbox"])
```

2. æ™ºèƒ½é¡µé¢æŠ“å–ï¼š
```python
def fetch_page(url):
    try:
        resp = session.get(url, 
                         headers={'User-Agent': fake_useragent()},
                         proxies={'http': get_proxy()},
                         timeout=10)
        resp.html.render(sleep=2, scrolldown=3)  # æ¨¡æ‹Ÿæ»šåŠ¨åŠ è½½
        return resp.html
    except Exception as e:
        log_error(f"æŠ“å–å¤±è´¥: {str(e)}")
        return None
```

3. æ•°æ®è§£æï¼ˆç¤ºä¾‹äº¬ä¸œä»·æ ¼ï¼‰ï¼š
```python
def parse_jd(html):
    return {
        'price': html.find('span.price')[0].text,
        'title': html.find('div.sku-name')[0].text,
        'rating': html.search('star-{}')[0]  # ä½¿ç”¨æ¨¡æ¿åŒ¹é…
    }
```

---

#### **2. æ•°æ®å­˜å‚¨æ¨¡å—**
**æŠ€æœ¯æ ˆ**ï¼š  
- `SQLite3`ï¼ˆè½»é‡çº§æ•°æ®åº“ï¼‰
- `Dataset`ï¼ˆç®€åŒ–SQLæ“ä½œåº“ï¼‰

**å…³é”®æ“ä½œ**ï¼š  
1. æ•°æ®åº“åˆå§‹åŒ–ï¼š
```python
import dataset
db = dataset.connect('sqlite:///prices.db')

def init_db():
    db.create_table('products', primary_id='sku_id', primary_type='String')
    db['products'].create_column('platform', db.types.string)
    db['products'].create_column('price_history', db.types.json)
```

2. æ•°æ®å†™å…¥ï¼š
```python
def save_data(item):
    db['products'].upsert({
        'sku_id': item['id'],
        'platform': 'JD',
        'price_history': json.dumps(item['prices']),
        'last_updated': datetime.now()
    }, ['sku_id'])
```

---

#### **3. åçˆ¬ç³»ç»Ÿ**
**æŠ€æœ¯æ ˆ**ï¼š  
- `time` + `random`ï¼ˆè¯·æ±‚é¢‘ç‡æ§åˆ¶ï¼‰
- `Faker`ï¼ˆç”Ÿæˆè™šå‡è¡Œä¸ºï¼‰

**å…³é”®æ“ä½œ**ï¼š  
1. è¡Œä¸ºä¼ªè£…ï¼š
```python
def human_like_action(element):
    # æ¨¡æ‹Ÿäººå·¥è¾“å…¥
    element.click()
    time.sleep(random.uniform(0.1, 0.3))
    element.send_keys(Keys.DOWN * 2)
```

2. è¯·æ±‚é—´éš”æ§åˆ¶ï¼š
```python
def smart_delay():
    delay = abs(np.random.normal(2.5, 1.2))  # æ­£æ€åˆ†å¸ƒå»¶è¿Ÿ
    time.sleep(delay)
```

---

#### **4. æ•°æ®åˆ†ææ¨¡å—**
**æŠ€æœ¯æ ˆ**ï¼š  
- `Pandas`ï¼ˆæ•°æ®å¤„ç†ï¼‰
- `NumPy`ï¼ˆæ•°å€¼è®¡ç®—ï¼‰

**å…³é”®æ“ä½œ**ï¼š  
1. æ•°æ®åŠ è½½ï¼š
```python
def load_data():
    df = pd.read_sql('SELECT * FROM products', db.engine)
    df['price_history'] = df['price_history'].apply(json.loads)
    return df.explode('price_history')
```

2. æ³¢åŠ¨åˆ†æï¼š
```python
def analyze_trend(df):
    return df.groupby('sku_id')['price'].agg([
        ('current', 'last'),
        ('7d_avg', lambda x: x[-7:].mean()),
        ('30d_volatility', lambda x: x.std()/x.mean())
    ])
```

---

#### **5. å¯è§†åŒ–æ¨¡å—**
**æŠ€æœ¯æ ˆ**ï¼š  
- `Matplotlib`ï¼ˆåŸºç¡€å›¾è¡¨ï¼‰
- `Plotly`ï¼ˆäº¤äº’å¼å›¾è¡¨ï¼‰

**å…³é”®æ“ä½œ**ï¼š  
1. ç”Ÿæˆä»·æ ¼æ›²çº¿ï¼š
```python
def plot_price(df):
    fig = px.line(df, x='date', y='price', 
                 color='platform', 
                 title='ä»·æ ¼è¶‹åŠ¿å¯¹æ¯”')
    fig.write_html('trend.html')
```

2. å¹³å°å¯¹æ¯”å›¾ï¼š
```python
plt.boxplot([jd_prices, amazon_prices],
           labels=['äº¬ä¸œ', 'äºšé©¬é€Š'])
plt.savefig('price_compare.png')
```

---

### **âš™ï¸ ç³»ç»Ÿé›†æˆæ“ä½œ**

#### **å®šæ—¶ä»»åŠ¡**
```python
from apscheduler.schedulers.blocking import BlockingScheduler

sched = BlockingScheduler()

@sched.scheduled_job('cron', hour=3)  # æ¯å¤©å‡Œæ™¨3ç‚¹æ‰§è¡Œ
def daily_task():
    fetch_data()
    analyze_data()
    send_report()

sched.start()
```

---

### **ğŸ” æŠ€æœ¯é€‰å‹å¯¹ç…§è¡¨**
| æ¨¡å—     | åŸºç¡€ç‰ˆæŠ€æœ¯    | è¿›é˜¶ç‰ˆæŠ€æœ¯          | æ ¸å¿ƒåŒºåˆ«     |
| -------- | ------------- | ------------------- | ------------ |
| æ•°æ®æŠ“å– | Requests-HTML | Scrapy + Playwright | å¹¶å‘å¤„ç†èƒ½åŠ› |
| æ•°æ®å­˜å‚¨ | SQLite        | MySQL + Redis       | æ‰©å±•æ€§ä¸æ€§èƒ½ |
| å¯è§†åŒ–   | Matplotlib    | Plotly Dash         | äº¤äº’æ€§       |
| éƒ¨ç½²     | æœ¬åœ°è„šæœ¬      | Docker Swarm        | ç¯å¢ƒä¸€è‡´æ€§   |

---

### **ğŸš¦ å¼€å‘æ£€æŸ¥æ¸…å•**
1. æ•°æ®æŠ“å–ï¼š
   - [ ] é¡µé¢å…ƒç´ å®šä½å‡†ç¡®ç‡ â‰¥95%
   - [ ] å¼‚å¸¸é‡è¯•æœºåˆ¶ï¼ˆ3æ¬¡é‡è¯•ï¼‰
   
2. æ•°æ®å­˜å‚¨ï¼š
   - [ ] å†å²æ•°æ®ç‰ˆæœ¬æ§åˆ¶
   - [ ] æ•°æ®å»é‡æœºåˆ¶

3. åçˆ¬ç­–ç•¥ï¼š
   - [ ] è¯·æ±‚å¤´éšæœºåŒ–æµ‹è¯•
   - [ ] IPå°ç¦æ£€æµ‹è„šæœ¬

---

**å…³é”®æç¤º**ï¼šæ¯ä¸ªæ¨¡å—å¼€å‘å®Œæˆåï¼Œç«‹å³è¿›è¡Œï¼š  
1. å•å…ƒæµ‹è¯•ï¼ˆpytestï¼‰  
2. æ€§èƒ½åŸºå‡†æµ‹è¯•  
3. å†…å­˜æ³„æ¼æ£€æŸ¥ï¼ˆmemory-profilerï¼‰  

è¿™ä¸ªç»“æ„åŒ–çš„æŠ€æœ¯æ€»ç»“èƒ½å¸®åŠ©ä½ é«˜æ•ˆæ¨è¿›é¡¹ç›®ï¼Œå»ºè®®æ‰“å°ä½œä¸ºå¼€å‘è·¯çº¿å›¾å‚è€ƒã€‚